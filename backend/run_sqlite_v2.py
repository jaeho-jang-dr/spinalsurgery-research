#!/usr/bin/env python3
"""
í–¥ìƒëœ SQLite ê¸°ë°˜ ë°±ì—”ë“œ ì„œë²„
ë…¼ë¬¸ ê²€ìƒ‰ ê¸°ëŠ¥ ì¶”ê°€
"""

import sqlite3
import json
from http.server import HTTPServer, BaseHTTPRequestHandler
from urllib.parse import urlparse, parse_qs
import hashlib
import uuid
from datetime import datetime
import sys
import os

# paper_search_service ì„í¬íŠ¸
sys.path.append(os.path.dirname(os.path.abspath(__file__)))
from paper_search_service import PaperSearchService
from search_engine import SearchEngine
from ai_service import AIService
import asyncio

# SQLite ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”
def init_db():
    conn = sqlite3.connect('spinalsurgery_research.db')
    c = conn.cursor()
    
    # Users í…Œì´ë¸”
    c.execute('''CREATE TABLE IF NOT EXISTS users (
        id TEXT PRIMARY KEY,
        email TEXT UNIQUE NOT NULL,
        password_hash TEXT NOT NULL,
        name TEXT NOT NULL,
        role TEXT NOT NULL,
        institution TEXT,
        department TEXT,
        created_at TEXT DEFAULT CURRENT_TIMESTAMP
    )''')
    
    # Projects í…Œì´ë¸”
    c.execute('''CREATE TABLE IF NOT EXISTS projects (
        id TEXT PRIMARY KEY,
        user_id TEXT NOT NULL,
        title TEXT NOT NULL,
        field TEXT NOT NULL,
        keywords TEXT,
        description TEXT,
        status TEXT DEFAULT 'draft',
        created_at TEXT DEFAULT CURRENT_TIMESTAMP,
        FOREIGN KEY (user_id) REFERENCES users (id)
    )''')
    
    # Papers í…Œì´ë¸”
    c.execute('''CREATE TABLE IF NOT EXISTS papers (
        id TEXT PRIMARY KEY,
        project_id TEXT,
        title TEXT NOT NULL,
        authors TEXT,
        abstract TEXT,
        journal_name TEXT,
        publication_year INTEGER,
        is_own_paper INTEGER DEFAULT 0,
        created_at TEXT DEFAULT CURRENT_TIMESTAMP,
        FOREIGN KEY (project_id) REFERENCES projects (id)
    )''')
    
    # Paper Sources í…Œì´ë¸”
    c.execute('''CREATE TABLE IF NOT EXISTS paper_sources (
        id TEXT PRIMARY KEY,
        name TEXT NOT NULL,
        type TEXT NOT NULL,
        priority INTEGER DEFAULT 5,
        url TEXT,
        contact_email TEXT,
        contact_phone TEXT,
        address TEXT,
        notes TEXT,
        created_at TEXT DEFAULT CURRENT_TIMESTAMP
    )''')
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ì‚½ì…
    # í…ŒìŠ¤íŠ¸ ì‚¬ìš©ì
    test_user_id = str(uuid.uuid4())
    c.execute('''INSERT OR IGNORE INTO users (id, email, password_hash, name, role, institution, department)
                 VALUES (?, ?, ?, ?, ?, ?, ?)''',
              (test_user_id, 'test@example.com', hashlib.sha256('test1234'.encode()).hexdigest(),
               'í…ŒìŠ¤íŠ¸ ì‚¬ìš©ì', 'researcher', 'ì„œìš¸ëŒ€í•™êµë³‘ì›', 'ì²™ì¶”ì™¸ê³¼'))
    
    # í…ŒìŠ¤íŠ¸ í”„ë¡œì íŠ¸
    c.execute('''INSERT OR IGNORE INTO projects (id, user_id, title, field, keywords, status)
                 VALUES (?, ?, ?, ?, ?, ?)''',
              ('1', test_user_id, 'ì²™ì¶” í›„ì™¸ë°© ê³ ì •ìˆ ì˜ 2ë…„ í›„ ê²°ê³¼', 'ì²™ì¶”ì™¸ê³¼', 
               '["ì²™ì¶”", "ê³ ì •ìˆ ", "CD instrument"]', 'in_progress'))
    
    c.execute('''INSERT OR IGNORE INTO projects (id, user_id, title, field, keywords, status)
                 VALUES (?, ?, ?, ?, ?, ?)''',
              ('2', test_user_id, 'ìµœì†Œ ì¹¨ìŠµ ì²™ì¶” ìˆ˜ìˆ ì˜ íš¨ê³¼ ë¶„ì„', 'ì²™ì¶”ì™¸ê³¼',
               '["ìµœì†Œì¹¨ìŠµ", "ì²™ì¶”ìˆ˜ìˆ ", "VAS score"]', 'draft'))
    
    # ë…¼ë¬¸ ì†ŒìŠ¤
    c.execute('''INSERT OR IGNORE INTO paper_sources (id, name, type, priority, url, contact_email, contact_phone, address)
                 VALUES (?, ?, ?, ?, ?, ?, ?, ?)''',
              ('1', 'PubMed Central', 'database', 1, 'https://www.ncbi.nlm.nih.gov/pmc/',
               'info@ncbi.nlm.nih.gov', None, None))
    
    c.execute('''INSERT OR IGNORE INTO paper_sources (id, name, type, priority, url, contact_email, contact_phone, address)
                 VALUES (?, ?, ?, ?, ?, ?, ?, ?)''',
              ('2', 'ì„œìš¸ëŒ€í•™êµ ì˜í•™ë„ì„œê´€', 'institution', 1, 'http://medlib.snu.ac.kr',
               'medlib@snu.ac.kr', '02-740-8045', 'ì„œìš¸íŠ¹ë³„ì‹œ ì¢…ë¡œêµ¬ ëŒ€í•™ë¡œ 103'))
    
    conn.commit()
    conn.close()

class APIHandler(BaseHTTPRequestHandler):
    search_engine = None  # í´ë˜ìŠ¤ ë³€ìˆ˜ë¡œ ê²€ìƒ‰ ì—”ì§„ ê³µìœ 
    ai_service = None  # AI ì„œë¹„ìŠ¤ ê³µìœ 
    
    def _set_headers(self, status=200):
        self.send_response(status)
        self.send_header('Content-type', 'application/json')
        self.send_header('Access-Control-Allow-Origin', '*')
        self.send_header('Access-Control-Allow-Methods', 'GET, POST, PUT, DELETE, OPTIONS')
        self.send_header('Access-Control-Allow-Headers', 'Content-Type, Authorization')
        self.send_header('Access-Control-Allow-Credentials', 'true')
        self.end_headers()
    
    def do_OPTIONS(self):
        self._set_headers(200)
    
    def do_GET(self):
        parsed_path = urlparse(self.path)
        path = parsed_path.path
        
        conn = sqlite3.connect('spinalsurgery_research.db')
        conn.row_factory = sqlite3.Row
        c = conn.cursor()
        
        try:
            if path == '/api/v1/users/me':
                self._set_headers()
                response = {
                    "id": "test-user-id",
                    "email": "test@example.com",
                    "name": "í…ŒìŠ¤íŠ¸ ì‚¬ìš©ì",
                    "role": "researcher",
                    "institution": "ì„œìš¸ëŒ€í•™êµë³‘ì›",
                    "department": "ì²™ì¶”ì™¸ê³¼"
                }
                self.wfile.write(json.dumps(response).encode())
            
            elif path == '/api/v1/projects':
                c.execute('''SELECT p.*, 
                            (SELECT COUNT(*) FROM papers WHERE project_id = p.id) as papers_count
                            FROM projects p''')
                projects = []
                for row in c.fetchall():
                    project = dict(row)
                    project['keywords'] = json.loads(project.get('keywords', '[]'))
                    project['patients_count'] = 34 if project['id'] == '1' else 0
                    project['collaborators_count'] = 3 if project['id'] == '1' else 1
                    project['created_at'] = project.get('created_at', '2025-01-15T10:00:00')
                    project['updated_at'] = project.get('created_at', '2025-01-20T15:30:00')
                    projects.append(project)
                
                self._set_headers()
                self.wfile.write(json.dumps(projects).encode())
            
            elif path == '/api/v1/papers/sources':
                c.execute('SELECT * FROM paper_sources ORDER BY priority, name')
                sources = [dict(row) for row in c.fetchall()]
                self._set_headers()
                self.wfile.write(json.dumps(sources).encode())
            
            elif path == '/api/v1/papers/search-sites':
                # ë…¼ë¬¸ ê²€ìƒ‰ ì‚¬ì´íŠ¸ ëª©ë¡
                search_service = PaperSearchService()
                sites = search_service.get_search_sites()
                self._set_headers()
                self.wfile.write(json.dumps(sites).encode())
            
            elif path.startswith('/api/v1/projects/') and path.endswith('/search-sessions'):
                # í”„ë¡œì íŠ¸ì˜ ê²€ìƒ‰ ì„¸ì…˜ ëª©ë¡
                project_id = path.split('/')[-2]
                c.execute('''SELECT s.*, j.status as job_status, j.progress, j.total_expected
                            FROM search_sessions s
                            LEFT JOIN search_jobs j ON s.id = j.session_id
                            WHERE s.project_id = ? 
                            ORDER BY s.started_at DESC''', (project_id,))
                sessions = []
                for row in c.fetchall():
                    session = dict(row)
                    session['search_sites'] = json.loads(session.get('search_sites', '[]'))
                    sessions.append(session)
                
                self._set_headers()
                self.wfile.write(json.dumps(sessions).encode())
            
            elif path.startswith('/api/v1/search/jobs/'):
                # ê²€ìƒ‰ ì‘ì—… ìƒíƒœ ì¡°íšŒ
                job_id = path.split('/')[-1]
                job_info = self.search_engine.get_job_info(job_id)
                
                if job_info:
                    self._set_headers()
                    self.wfile.write(json.dumps(job_info).encode())
                else:
                    self._set_headers(404)
                    self.wfile.write(json.dumps({"error": "Job not found"}).encode())
            
            elif path == '/api/v1/search/papers':
                # ì €ì¥ëœ ë…¼ë¬¸ì—ì„œ ê²€ìƒ‰
                query_params = parse_qs(parsed_path.query)
                query = query_params.get('q', [''])[0]
                project_id = query_params.get('project_id', [None])[0]
                
                if query:
                    results = self.search_engine.search_in_papers(query, project_id)
                    self._set_headers()
                    self.wfile.write(json.dumps(results).encode())
                else:
                    self._set_headers(400)
                    self.wfile.write(json.dumps({"error": "Query parameter required"}).encode())
            
            elif path == '/':
                self._set_headers()
                response = {"message": "SpinalSurgery Research Platform API - SQLite Backend v2"}
                self.wfile.write(json.dumps(response).encode())
            
            else:
                self._set_headers(404)
                self.wfile.write(json.dumps({"error": "Not found"}).encode())
                
        finally:
            conn.close()
    
    def do_POST(self):
        content_length = int(self.headers.get('Content-Length', 0))
        post_data = self.rfile.read(content_length) if content_length > 0 else b'{}'
        
        parsed_path = urlparse(self.path)
        path = parsed_path.path
        
        if path == '/api/v1/auth/login':
            self._set_headers()
            response = {
                "access_token": "test-access-token-" + str(uuid.uuid4()),
                "refresh_token": "test-refresh-token-" + str(uuid.uuid4()),
                "token_type": "bearer"
            }
            self.wfile.write(json.dumps(response).encode())
        
        elif path == '/api/v1/projects':
            try:
                data = json.loads(post_data)
                conn = sqlite3.connect('spinalsurgery_research.db')
                c = conn.cursor()
                
                project_id = str(uuid.uuid4())
                c.execute('''INSERT INTO projects (id, user_id, title, field, keywords, description, status)
                            VALUES (?, ?, ?, ?, ?, ?, ?)''',
                         (project_id, 'test-user-id', data.get('title'), data.get('field'),
                          json.dumps(data.get('keywords', [])), data.get('description'), 'draft'))
                conn.commit()
                conn.close()
                
                self._set_headers(201)
                response = {
                    "id": project_id,
                    "title": data.get('title'),
                    "field": data.get('field'),
                    "keywords": data.get('keywords', []),
                    "description": data.get('description'),
                    "status": "draft",
                    "papers_count": 0,
                    "patients_count": 0,
                    "collaborators_count": 0,
                    "created_at": datetime.now().isoformat()
                }
                self.wfile.write(json.dumps(response).encode())
                
            except Exception as e:
                self._set_headers(400)
                self.wfile.write(json.dumps({"error": str(e)}).encode())
        
        elif path == '/api/v1/papers/search':
            try:
                data = json.loads(post_data)
                project_id = data.get('project_id')
                query = data.get('query')
                site_ids = data.get('site_ids', ['pubmed'])
                target_count = data.get('target_count', 100)
                
                # ëŒ€ëŸ‰ ë…¼ë¬¸ ê²€ìƒ‰ ì‹œì‘
                job_id = self.search_engine.start_search(project_id, query, site_ids, target_count)
                
                self._set_headers()
                response = {
                    "job_id": job_id,
                    "message": f"ê²€ìƒ‰ ì‘ì—…ì´ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤. ëª©í‘œ: {target_count}ê°œ ë…¼ë¬¸"
                }
                self.wfile.write(json.dumps(response).encode())
                
            except Exception as e:
                self._set_headers(400)
                self.wfile.write(json.dumps({"error": str(e)}).encode())
        
        elif path.startswith('/api/v1/search/jobs/') and path.endswith('/pause'):
            # ê²€ìƒ‰ ì¼ì‹œì •ì§€
            job_id = path.split('/')[-2]
            self.search_engine.pause_search(job_id)
            self._set_headers()
            self.wfile.write(json.dumps({"status": "paused"}).encode())
        
        elif path.startswith('/api/v1/search/jobs/') and path.endswith('/resume'):
            # ê²€ìƒ‰ ì¬ê°œ
            job_id = path.split('/')[-2]
            self.search_engine.resume_search(job_id)
            self._set_headers()
            self.wfile.write(json.dumps({"status": "resumed"}).encode())
        
        elif path.startswith('/api/v1/search/jobs/') and path.endswith('/cancel'):
            # ê²€ìƒ‰ ì·¨ì†Œ
            job_id = path.split('/')[-2]
            self.search_engine.cancel_search(job_id)
            self._set_headers()
            self.wfile.write(json.dumps({"status": "cancelled"}).encode())
        
        elif path.startswith('/api/v1/projects/') and path.endswith('/start-research'):
            try:
                project_id = path.split('/')[-2]
                data = json.loads(post_data)
                
                # í”„ë¡œì íŠ¸ ì •ë³´ ì¡°íšŒ
                conn = sqlite3.connect('spinalsurgery_research.db')
                conn.row_factory = sqlite3.Row
                c = conn.cursor()
                
                c.execute('SELECT * FROM projects WHERE id = ?', (project_id,))
                project = dict(c.fetchone())
                conn.close()
                
                # ê²€ìƒ‰ì–´ ìƒì„± (ì˜ì–´ë¡œ ë³€í™˜)
                keywords = json.loads(project['keywords'])
                # í•œê¸€ í‚¤ì›Œë“œë¥¼ ì˜ì–´ë¡œ ë§¤í•‘ (ê°„ë‹¨í•œ ì˜ˆì‹œ)
                keyword_map = {
                    'ì²™ì¶”': 'spine',
                    'ì²™ì¶”ì™¸ê³¼': 'spine surgery',
                    'ê³ ì •ìˆ ': 'fusion',
                    'ìµœì†Œì¹¨ìŠµ': 'minimally invasive',
                    'ì²™ì¶”ìˆ˜ìˆ ': 'spine surgery',
                    'VAS score': 'VAS score',
                    'CD instrument': 'CD instrument'
                }
                
                # ì˜ì–´ í‚¤ì›Œë“œë¡œ ë³€í™˜
                english_keywords = []
                for keyword in keywords:
                    english_keywords.append(keyword_map.get(keyword, keyword))
                
                field_english = keyword_map.get(project['field'], project['field'])
                query = f"{field_english} {' '.join(english_keywords)}"
                
                # AI ì˜µì…˜ì— ë”°ë¥¸ ì²˜ë¦¬
                ai_option = data.get('ai_option', 'search')
                
                if ai_option == 'search':
                    # ëŒ€ëŸ‰ ë…¼ë¬¸ ê²€ìƒ‰ ì‹œì‘ (ë°±ê·¸ë¼ìš´ë“œ)
                    job_id = self.search_engine.start_search(
                        project_id, 
                        query, 
                        data.get('site_ids', ['pubmed', 'pmc']),
                        target_count=data.get('target_count', 100)
                    )
                    
                    self._set_headers()
                    response = {
                        "status": "success",
                        "action": "search_started",
                        "job_id": job_id,
                        "message": "ë…¼ë¬¸ ê²€ìƒ‰ì´ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤. ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì§„í–‰ë©ë‹ˆë‹¤."
                    }
                    self.wfile.write(json.dumps(response).encode())
                
                else:
                    # TODO: ë‹¤ë¥¸ AI ì˜µì…˜ êµ¬í˜„
                    self._set_headers()
                    response = {
                        "status": "pending",
                        "action": ai_option,
                        "message": f"{ai_option} ê¸°ëŠ¥ì€ ì¤€ë¹„ ì¤‘ì…ë‹ˆë‹¤"
                    }
                    self.wfile.write(json.dumps(response).encode())
                    
            except Exception as e:
                self._set_headers(400)
                self.wfile.write(json.dumps({"error": str(e)}).encode())
        
        elif path == '/api/v1/ai/models':
            # ì‚¬ìš© ê°€ëŠ¥í•œ AI ëª¨ë¸ ëª©ë¡
            try:
                ollama_models = self.ai_service.list_ollama_models()
                models = [
                    {'id': 'claude-3-opus', 'name': 'Claude 3 Opus', 'type': 'claude', 'available': bool(self.ai_service.claude_api_key)},
                    {'id': 'claude-3-sonnet', 'name': 'Claude 3 Sonnet', 'type': 'claude', 'available': bool(self.ai_service.claude_api_key)},
                ]
                for model in ollama_models:
                    models.append({'id': model, 'name': model, 'type': 'ollama', 'available': True})
                
                self._set_headers()
                self.wfile.write(json.dumps(models).encode())
                
            except Exception as e:
                self._set_headers(500)
                self.wfile.write(json.dumps({"error": str(e)}).encode())
        
        elif path == '/api/v1/ai/chat':
            # AI ì±„íŒ…
            try:
                data = json.loads(post_data)
                project_id = data.get('project_id')
                message = data.get('message')
                model = data.get('model', 'llama2')
                session_id = data.get('session_id')
                
                if not session_id:
                    session_id = self.ai_service.create_ai_session(project_id, 'chat', model)
                
                # ë©”ì‹œì§€ ì €ì¥
                self.ai_service.save_conversation(session_id, 'user', message)
                
                # AI ì‘ë‹µ ìƒì„±
                loop = asyncio.new_event_loop()
                asyncio.set_event_loop(loop)
                result = loop.run_until_complete(
                    self.ai_service.query_ollama(message, model)
                )
                
                if 'response' in result:
                    response_text = result['response']
                    self.ai_service.save_conversation(session_id, 'assistant', response_text)
                    
                    self._set_headers()
                    self.wfile.write(json.dumps({
                        'session_id': session_id,
                        'response': response_text,
                        'model': model
                    }).encode())
                else:
                    self._set_headers(500)
                    self.wfile.write(json.dumps(result).encode())
                    
            except Exception as e:
                self._set_headers(400)
                self.wfile.write(json.dumps({"error": str(e)}).encode())
        
        elif path == '/api/v1/ai/analyze-documents':
            # ë¬¸ì„œ ë¶„ì„
            try:
                data = json.loads(post_data)
                project_id = data.get('project_id')
                document_paths = data.get('document_paths', [])
                analysis_type = data.get('analysis_type', 'summary')
                model = data.get('model', 'llama2')
                
                loop = asyncio.new_event_loop()
                asyncio.set_event_loop(loop)
                result = loop.run_until_complete(
                    self.ai_service.analyze_documents(project_id, document_paths, analysis_type, model)
                )
                
                self._set_headers()
                self.wfile.write(json.dumps(result).encode())
                
            except Exception as e:
                self._set_headers(400)
                self.wfile.write(json.dumps({"error": str(e)}).encode())
        
        elif path == '/api/v1/ai/generate-draft':
            # ë…¼ë¬¸ ì´ˆì•ˆ ìƒì„±
            try:
                data = json.loads(post_data)
                project_id = data.get('project_id')
                title = data.get('title')
                keywords = data.get('keywords', [])
                outline = data.get('outline', {})
                references = data.get('references', [])
                model = data.get('model', 'llama2')
                
                loop = asyncio.new_event_loop()
                asyncio.set_event_loop(loop)
                result = loop.run_until_complete(
                    self.ai_service.generate_paper_draft(
                        project_id, title, keywords, outline, references, model
                    )
                )
                
                self._set_headers()
                self.wfile.write(json.dumps(result).encode())
                
            except Exception as e:
                self._set_headers(400)
                self.wfile.write(json.dumps({"error": str(e)}).encode())
        
        else:
            self._set_headers(404)
            self.wfile.write(json.dumps({"error": "Not found"}).encode())

def run(port=8000):
    # ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”
    init_db()
    
    # ë…¼ë¬¸ ê²€ìƒ‰ ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
    search_service = PaperSearchService()
    search_engine = SearchEngine()
    
    # AI ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
    ai_service = AIService()
    
    # APIHandlerì— ì„œë¹„ìŠ¤ ì£¼ì…
    APIHandler.search_engine = search_engine
    APIHandler.ai_service = ai_service
    
    # ì„œë²„ ì‹œì‘
    server_address = ('', port)
    httpd = HTTPServer(server_address, APIHandler)
    
    print(f'ğŸš€ SpinalSurgery Research Backend v2 (SQLite + Paper Search)')
    print(f'ğŸ“¦ Database: spinalsurgery_research.db')
    print(f'ğŸŒ Server running on http://localhost:{port}')
    print(f'ğŸ“‹ API endpoints:')
    print(f'   GET  /api/v1/users/me')
    print(f'   GET  /api/v1/projects')
    print(f'   POST /api/v1/projects')
    print(f'   GET  /api/v1/papers/sources')
    print(f'   GET  /api/v1/papers/search-sites')
    print(f'   POST /api/v1/papers/search - ëŒ€ëŸ‰ ê²€ìƒ‰ ì‹œì‘')
    print(f'   GET  /api/v1/search/papers?q=query - ì €ì¥ëœ ë…¼ë¬¸ ê²€ìƒ‰')
    print(f'   GET  /api/v1/search/jobs/:id - ê²€ìƒ‰ ì‘ì—… ìƒíƒœ')
    print(f'   POST /api/v1/search/jobs/:id/pause - ê²€ìƒ‰ ì¼ì‹œì •ì§€')
    print(f'   POST /api/v1/search/jobs/:id/resume - ê²€ìƒ‰ ì¬ê°œ')
    print(f'   POST /api/v1/search/jobs/:id/cancel - ê²€ìƒ‰ ì·¨ì†Œ')
    print(f'   GET  /api/v1/projects/:id/search-sessions - ê²€ìƒ‰ ì„¸ì…˜ ëª©ë¡')
    print(f'   POST /api/v1/projects/:id/start-research')
    print(f'   POST /api/v1/auth/login')
    print(f'   POST /api/v1/ai/models - AI ëª¨ë¸ ëª©ë¡')
    print(f'   POST /api/v1/ai/chat - AI ì±„íŒ…')
    print(f'   POST /api/v1/ai/analyze-documents - ë¬¸ì„œ ë¶„ì„')
    print(f'   POST /api/v1/ai/generate-draft - ë…¼ë¬¸ ì´ˆì•ˆ ìƒì„±')
    print(f'\n Press Ctrl+C to stop')
    
    try:
        httpd.serve_forever()
    except KeyboardInterrupt:
        print('\nğŸ›‘ Server stopped')

if __name__ == '__main__':
    run()