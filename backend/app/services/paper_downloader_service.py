"""
Paper Download Service - ì‹¤ì œ ë…¼ë¬¸ ë‹¤ìš´ë¡œë“œ ë° ì²˜ë¦¬
"""
import os
import asyncio
import aiohttp
import requests
from typing import Dict, List, Optional, Tuple
from datetime import datetime
import xml.etree.ElementTree as ET
import PyPDF2
import pdfplumber
from deep_translator import GoogleTranslator
import hashlib
import json
from pathlib import Path
import re

class PaperDownloaderService:
    def __init__(self):
        self.base_url = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
        self.pmc_base = "https://www.ncbi.nlm.nih.gov/pmc/articles"
        self.pubmed_base = "https://pubmed.ncbi.nlm.nih.gov"
        
        # ì €ì¥ ê²½ë¡œ
        self.storage_path = Path("/home/drjang00/DevEnvironments/spinalsurgery-research/downloaded_papers")
        self.storage_path.mkdir(exist_ok=True)
        
        # ë²ˆì—­ê¸°
        self.translator = GoogleTranslator(source='en', target='ko')
        
        # API í‚¤ (í•„ìš”ì‹œ)
        self.api_key = os.getenv("PUBMED_API_KEY", "")
        
    async def search_and_download_papers(
        self, 
        query: str, 
        max_results: int = 5,
        translate_to_korean: bool = True
    ) -> List[Dict]:
        """ë…¼ë¬¸ ê²€ìƒ‰ ë° ë‹¤ìš´ë¡œë“œ ë©”ì¸ í•¨ìˆ˜"""
        print(f"ğŸ” Searching for: {query}")
        
        # 1. PubMed ê²€ìƒ‰
        pmids = await self._search_pubmed(query, max_results)
        if not pmids:
            print("âŒ No papers found")
            return []
            
        print(f"ğŸ“„ Found {len(pmids)} papers")
        
        # 2. ê° ë…¼ë¬¸ ì²˜ë¦¬
        results = []
        for i, pmid in enumerate(pmids, 1):
            print(f"\n--- Processing paper {i}/{len(pmids)} (PMID: {pmid}) ---")
            
            # ë…¼ë¬¸ ë©”íƒ€ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
            metadata = await self._fetch_paper_metadata(pmid)
            if not metadata:
                continue
                
            # í´ë” ìƒì„±
            paper_folder = self._create_paper_folder(metadata)
            
            # ë…¼ë¬¸ ë‹¤ìš´ë¡œë“œ ì‹œë„
            pdf_path = await self._download_paper_pdf(pmid, metadata, paper_folder)
            
            # PDFì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            if pdf_path and pdf_path.exists():
                full_text = self._extract_text_from_pdf(pdf_path)
                metadata['full_text'] = full_text
            else:
                # PDFê°€ ì—†ìœ¼ë©´ abstractë§Œ ì‚¬ìš©
                full_text = metadata.get('abstract', '')
                
            # í•œê¸€ ë²ˆì—­
            if translate_to_korean and full_text:
                print("ğŸŒ Translating to Korean...")
                metadata['korean_translation'] = await self._translate_to_korean(
                    metadata, full_text
                )
                
            # ë©”íƒ€ë°ì´í„° ì €ì¥
            self._save_metadata(metadata, paper_folder)
            
            # ìš”ì•½ ìƒì„±
            summary = self._create_summary(metadata, paper_folder)
            
            results.append({
                'pmid': pmid,
                'metadata': metadata,
                'folder': str(paper_folder),
                'summary': summary,
                'pdf_downloaded': pdf_path is not None and pdf_path.exists()
            })
            
        return results
        
    async def _search_pubmed(self, query: str, max_results: int) -> List[str]:
        """PubMed ê²€ìƒ‰"""
        search_url = f"{self.base_url}/esearch.fcgi"
        params = {
            'db': 'pubmed',
            'term': query,
            'retmax': max_results,
            'retmode': 'json'
        }
        
        if self.api_key:
            params['api_key'] = self.api_key
            
        try:
            async with aiohttp.ClientSession() as session:
                async with session.get(search_url, params=params) as response:
                    if response.status == 200:
                        data = await response.json()
                        return data.get('esearchresult', {}).get('idlist', [])
                    else:
                        print(f"Search error: {response.status}")
                        text = await response.text()
                        print(f"Response: {text[:200]}")
        except Exception as e:
            print(f"Exception during search: {e}")
        return []
        
    async def _fetch_paper_metadata(self, pmid: str) -> Optional[Dict]:
        """ë…¼ë¬¸ ë©”íƒ€ë°ì´í„° ê°€ì ¸ì˜¤ê¸°"""
        fetch_url = f"{self.base_url}/efetch.fcgi"
        params = {
            'db': 'pubmed',
            'id': pmid,
            'rettype': 'abstract',
            'retmode': 'xml'
        }
        
        if self.api_key:
            params['api_key'] = self.api_key
            
        async with aiohttp.ClientSession() as session:
            async with session.get(fetch_url, params=params) as response:
                if response.status == 200:
                    xml_data = await response.text()
                    return self._parse_pubmed_xml(xml_data, pmid)
        return None
        
    def _parse_pubmed_xml(self, xml_data: str, pmid: str) -> Dict:
        """PubMed XML íŒŒì‹±"""
        root = ET.fromstring(xml_data)
        article = root.find('.//PubmedArticle')
        
        if not article:
            return {}
            
        metadata = {'pmid': pmid}
        
        # Title
        title = article.find('.//ArticleTitle')
        metadata['title'] = title.text if title is not None else 'Unknown Title'
        
        # Abstract
        abstract_texts = []
        for abstract in article.findall('.//AbstractText'):
            if abstract.text:
                label = abstract.get('Label', '')
                if label:
                    abstract_texts.append(f"{label}: {abstract.text}")
                else:
                    abstract_texts.append(abstract.text)
        metadata['abstract'] = '\n\n'.join(abstract_texts)
        
        # Authors
        authors = []
        for author in article.findall('.//Author'):
            last_name = author.find('LastName')
            fore_name = author.find('ForeName')
            if last_name is not None and fore_name is not None:
                authors.append(f"{fore_name.text} {last_name.text}")
        metadata['authors'] = authors
        
        # Journal
        journal = article.find('.//Journal/Title')
        metadata['journal'] = journal.text if journal is not None else ''
        
        # Year
        year = article.find('.//PubDate/Year')
        metadata['year'] = year.text if year is not None else ''
        
        # DOI
        doi = article.find('.//ArticleId[@IdType="doi"]')
        metadata['doi'] = doi.text if doi is not None else ''
        
        # PMC ID
        pmc = article.find('.//ArticleId[@IdType="pmc"]')
        metadata['pmc_id'] = pmc.text if pmc is not None else ''
        
        # Keywords
        keywords = []
        for keyword in article.findall('.//Keyword'):
            if keyword.text:
                keywords.append(keyword.text)
        metadata['keywords'] = keywords
        
        return metadata
        
    def _create_paper_folder(self, metadata: Dict) -> Path:
        """ë…¼ë¬¸ë³„ í´ë” ìƒì„±"""
        # ì•ˆì „í•œ íŒŒì¼ëª… ìƒì„±
        safe_title = re.sub(r'[^\w\s-]', '', metadata['title'])[:50]
        safe_title = re.sub(r'[-\s]+', '-', safe_title)
        
        folder_name = f"{metadata['pmid']}_{safe_title}"
        folder_path = self.storage_path / folder_name
        folder_path.mkdir(exist_ok=True)
        
        return folder_path
        
    async def _download_paper_pdf(
        self, 
        pmid: str, 
        metadata: Dict, 
        folder: Path
    ) -> Optional[Path]:
        """ë…¼ë¬¸ PDF ë‹¤ìš´ë¡œë“œ"""
        pdf_path = folder / f"{pmid}.pdf"
        
        # PMC IDê°€ ìˆëŠ” ê²½ìš° PMCì—ì„œ ë‹¤ìš´ë¡œë“œ ì‹œë„
        if metadata.get('pmc_id'):
            pmc_pdf_url = f"{self.pmc_base}/{metadata['pmc_id']}/pdf/"
            if await self._download_file(pmc_pdf_url, pdf_path):
                print(f"âœ… Downloaded PDF from PMC")
                return pdf_path
                
        # DOIë¥¼ í†µí•œ ë‹¤ìš´ë¡œë“œ ì‹œë„ (Sci-Hub ë“± ì‚¬ìš© ê°€ëŠ¥)
        if metadata.get('doi'):
            # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ì ì ˆí•œ ë°©ë²• ì‚¬ìš©
            print(f"âš ï¸ PDF download via DOI not implemented (DOI: {metadata['doi']})")
            
        # PDFë¥¼ ì°¾ì„ ìˆ˜ ì—†ëŠ” ê²½ìš°
        print(f"âŒ Could not download PDF for PMID: {pmid}")
        return None
        
    async def _download_file(self, url: str, filepath: Path) -> bool:
        """íŒŒì¼ ë‹¤ìš´ë¡œë“œ"""
        try:
            async with aiohttp.ClientSession() as session:
                async with session.get(url) as response:
                    if response.status == 200:
                        content = await response.read()
                        with open(filepath, 'wb') as f:
                            f.write(content)
                        return True
        except Exception as e:
            print(f"Download error: {e}")
        return False
        
    def _extract_text_from_pdf(self, pdf_path: Path) -> str:
        """PDFì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        text = ""
        
        try:
            # pdfplumber ì‚¬ìš© (ë” ì •í™•í•œ í…ìŠ¤íŠ¸ ì¶”ì¶œ)
            with pdfplumber.open(pdf_path) as pdf:
                for page in pdf.pages:
                    page_text = page.extract_text()
                    if page_text:
                        text += page_text + "\n"
                        
            if not text.strip():
                # PyPDF2ë¡œ ì¬ì‹œë„
                with open(pdf_path, 'rb') as f:
                    pdf_reader = PyPDF2.PdfReader(f)
                    for page in pdf_reader.pages:
                        text += page.extract_text() + "\n"
                        
        except Exception as e:
            print(f"PDF extraction error: {e}")
            
        return text.strip()
        
    async def _translate_to_korean(self, metadata: Dict, full_text: str) -> Dict:
        """í•œê¸€ ë²ˆì—­"""
        korean_data = {}
        
        try:
            # ì œëª© ë²ˆì—­
            if metadata.get('title'):
                korean_data['title'] = self.translator.translate(metadata['title'])
                
            # ì´ˆë¡ ë²ˆì—­
            if metadata.get('abstract'):
                # ê¸´ í…ìŠ¤íŠ¸ëŠ” ë‚˜ëˆ„ì–´ ë²ˆì—­
                abstract_parts = self._split_text(metadata['abstract'], 4500)
                translated_parts = []
                
                for part in abstract_parts:
                    translated = self.translator.translate(part)
                    translated_parts.append(translated)
                    await asyncio.sleep(0.5)  # API ì œí•œ íšŒí”¼
                    
                korean_data['abstract'] = '\n'.join(translated_parts)
                
            # ì „ì²´ í…ìŠ¤íŠ¸ ë²ˆì—­ (ìš”ì•½ë³¸)
            if full_text:
                # ì „ì²´ í…ìŠ¤íŠ¸ëŠ” ë„ˆë¬´ ê¸¸ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì£¼ìš” ë¶€ë¶„ë§Œ ë²ˆì—­
                summary = self._extract_key_sections(full_text)
                
                if summary:
                    summary_parts = self._split_text(summary, 4500)
                    translated_summary = []
                    
                    for part in summary_parts:
                        translated = self.translator.translate(part)
                        translated_summary.append(translated)
                        await asyncio.sleep(0.5)
                        
                    korean_data['summary'] = '\n'.join(translated_summary)
                    
        except Exception as e:
            print(f"Translation error: {e}")
            
        return korean_data
        
    def _split_text(self, text: str, max_length: int) -> List[str]:
        """ê¸´ í…ìŠ¤íŠ¸ë¥¼ ì ì ˆí•œ ê¸¸ì´ë¡œ ë¶„í• """
        if len(text) <= max_length:
            return [text]
            
        parts = []
        sentences = text.split('. ')
        current_part = ""
        
        for sentence in sentences:
            if len(current_part) + len(sentence) + 2 <= max_length:
                current_part += sentence + ". "
            else:
                if current_part:
                    parts.append(current_part.strip())
                current_part = sentence + ". "
                
        if current_part:
            parts.append(current_part.strip())
            
        return parts
        
    def _extract_key_sections(self, full_text: str) -> str:
        """ì „ì²´ í…ìŠ¤íŠ¸ì—ì„œ ì£¼ìš” ì„¹ì…˜ ì¶”ì¶œ"""
        # ì£¼ìš” ì„¹ì…˜ íŒ¨í„´
        sections = {
            'introduction': r'(?i)introduction.*?(?=methods|materials)',
            'methods': r'(?i)(methods|materials).*?(?=results)',
            'results': r'(?i)results.*?(?=discussion|conclusion)',
            'conclusion': r'(?i)(conclusion|summary).*?(?=references|$)'
        }
        
        extracted = []
        
        for section_name, pattern in sections.items():
            match = re.search(pattern, full_text, re.DOTALL)
            if match:
                section_text = match.group(0)[:1000]  # ê° ì„¹ì…˜ë‹¹ ìµœëŒ€ 1000ì
                extracted.append(f"[{section_name.upper()}]\n{section_text}")
                
        return '\n\n'.join(extracted) if extracted else full_text[:4000]
        
    def _save_metadata(self, metadata: Dict, folder: Path):
        """ë©”íƒ€ë°ì´í„° ì €ì¥"""
        # JSON íŒŒì¼ë¡œ ì €ì¥
        json_path = folder / 'metadata.json'
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, ensure_ascii=False, indent=2)
            
        # í…ìŠ¤íŠ¸ íŒŒì¼ë¡œë„ ì €ì¥ (ì½ê¸° ì‰½ê²Œ)
        txt_path = folder / 'summary.txt'
        with open(txt_path, 'w', encoding='utf-8') as f:
            f.write(f"Title: {metadata.get('title', 'N/A')}\n")
            f.write(f"Authors: {', '.join(metadata.get('authors', []))}\n")
            f.write(f"Journal: {metadata.get('journal', 'N/A')}\n")
            f.write(f"Year: {metadata.get('year', 'N/A')}\n")
            f.write(f"DOI: {metadata.get('doi', 'N/A')}\n")
            f.write(f"PMID: {metadata.get('pmid', 'N/A')}\n")
            f.write(f"PMC ID: {metadata.get('pmc_id', 'N/A')}\n\n")
            
            f.write("=" * 80 + "\n")
            f.write("ABSTRACT\n")
            f.write("=" * 80 + "\n")
            f.write(metadata.get('abstract', 'No abstract available') + "\n\n")
            
            # í•œê¸€ ë²ˆì—­ì´ ìˆëŠ” ê²½ìš°
            if 'korean_translation' in metadata:
                f.write("=" * 80 + "\n")
                f.write("í•œê¸€ ë²ˆì—­\n")
                f.write("=" * 80 + "\n")
                
                korean = metadata['korean_translation']
                f.write(f"ì œëª©: {korean.get('title', 'N/A')}\n\n")
                f.write(f"ì´ˆë¡:\n{korean.get('abstract', 'N/A')}\n\n")
                
                if 'summary' in korean:
                    f.write(f"ì£¼ìš” ë‚´ìš© ìš”ì•½:\n{korean['summary']}\n")
                    
    def _create_summary(self, metadata: Dict, folder: Path) -> Dict:
        """ë…¼ë¬¸ ìš”ì•½ ìƒì„±"""
        summary = {
            'pmid': metadata['pmid'],
            'title': metadata['title'],
            'year': metadata['year'],
            'journal': metadata['journal'],
            'authors': metadata['authors'][:3] if metadata.get('authors') else [],
            'folder': str(folder),
            'has_pdf': (folder / f"{metadata['pmid']}.pdf").exists(),
            'has_translation': 'korean_translation' in metadata
        }
        
        # í•œê¸€ ì œëª© ì¶”ê°€
        if 'korean_translation' in metadata:
            summary['korean_title'] = metadata['korean_translation'].get('title', '')
            
        return summary

# ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤
paper_downloader_service = PaperDownloaderService()